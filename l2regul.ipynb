{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfnkw3R5yorQ",
    "outputId": "b2c94bed-e48b-418d-b99d-d9567a039ae6"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import imageio\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "R85orB7Xv6NH",
    "outputId": "e6fbfba0-d12f-43d5-d9f6-a4cecc466902"
   },
   "outputs": [],
   "source": [
    "# Creating a cvs file from the bpseq file\n",
    "def readfile(filename, rna_dir):\n",
    "    fopen = open(rna_dir + filename, 'r')\n",
    "    rnafile = fopen.readlines()\n",
    "    with open(\"test.txt\", 'w') as f:\n",
    "        for i in range(len(rnafile)):\n",
    "            s = rnafile[i]\n",
    "            result = '\\t'.join(s.split())\n",
    "            result = result + \"\\n\"\n",
    "            f.write(result)\n",
    "    data = pd.read_csv(\"test.txt\", sep='\\t', header=None)\n",
    "    return data, filename\n",
    "\n",
    "\n",
    "# Check for pseudknots if there is one presented then return true otherwise false\n",
    "def check_pseudoknot(data, filename):\n",
    "    rnadata1 = data.loc[:, 0]\n",
    "    rnadata2 = data.loc[:, 2]\n",
    "    flag = False\n",
    "    for i in range(len(rnadata2)):\n",
    "        for j in range(len(rnadata2)):\n",
    "            if (rnadata1[i] < rnadata1[j] < rnadata2[i] < rnadata2[j]):\n",
    "                flag = True\n",
    "                break\n",
    "    return flag\n",
    "\n",
    "# Removing files with pseudoknots\n",
    "def remove_pseudoknot(rna_dir):\n",
    "    pathDir = os.listdir(rna_dir)\n",
    "    for i in pathDir:\n",
    "        data, filename = readfile(i, rna_dir)\n",
    "        flag = check_pseudoknot(data, filename)\n",
    "        if flag:\n",
    "            os.remove(rna_dir + i)\n",
    "\n",
    "\n",
    "# Create dot-bracket structure\n",
    "def transform(data, filename):\n",
    "    rnaseq = data.loc[:, 1]\n",
    "    rnadata1 = data.loc[:, 0]\n",
    "    rnadata2 = data.loc[:, 2]\n",
    "    rnastructure = []\n",
    "    for i in range(len(rnadata2)):\n",
    "        if rnadata2[i] == 0:\n",
    "            rnastructure.append(\".\")\n",
    "        else:\n",
    "            if rnadata1[i] > rnadata2[i]:\n",
    "                rnastructure.append(\")\")\n",
    "            else:\n",
    "                rnastructure.append(\"(\")\n",
    "    return rnaseq, rnastructure, filename\n",
    "\n",
    "\n",
    "# Extract sequence and structure data\n",
    "def data_extract(rna_dir):\n",
    "    pathDir = os.listdir(rna_dir)\n",
    "    data_dict = {}\n",
    "    for i in pathDir:\n",
    "        data, filename = readfile(i, rna_dir)\n",
    "        rnaseq, rnastructure, filename = transform(data, filename)\n",
    "        data_dict[filename] = [rnaseq, rnastructure]\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def Gaussian(x):\n",
    "    return math.exp(-0.5*(x*x))\n",
    "\n",
    "# Gives value how likely a base pair happen\n",
    "def paired(x, y):\n",
    "    if x == 'A' and y == 'U':\n",
    "        return 2\n",
    "    elif x == 'G' and y == 'C':\n",
    "        return 3\n",
    "    elif x == \"G\" and y == 'U':\n",
    "        return 0.8\n",
    "    elif x == 'U' and y == 'A':\n",
    "        return 2\n",
    "    elif x == 'C' and y == 'G':\n",
    "        return 3\n",
    "    elif x == \"U\" and y == 'G':\n",
    "        return 0.8\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Representing the rna as matrix\n",
    "def creatmat(data):\n",
    "    mat = np.zeros([len(data), len(data)])\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            coefficient = 0\n",
    "            for add in range(30):\n",
    "                if i - add >= 0 and j + add < len(data):\n",
    "                    score = paired(data[i - add], data[j + add])\n",
    "                    if score == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        coefficient = coefficient + score * Gaussian(add)\n",
    "                else:\n",
    "                    break\n",
    "            if coefficient > 0:\n",
    "                for add in range(1, 30):\n",
    "                    if i + add < len(data) and j - add >= 0:\n",
    "                        score = paired(data[i + add], data[j - add])\n",
    "                        if score == 0:\n",
    "                            break\n",
    "                        else:\n",
    "                            coefficient = coefficient + score * Gaussian(add)\n",
    "                    else:\n",
    "                        break\n",
    "            mat[[i], [j]] = coefficient\n",
    "    return mat\n",
    "\n",
    "# Helper founction for file name giving\n",
    "def complete(i):\n",
    "    if i < 10:\n",
    "        str1 = '00' + str(i)\n",
    "    elif i < 100:\n",
    "        str1 = '0' + str(i)\n",
    "    else:\n",
    "        str1 = str(i)\n",
    "    return str1\n",
    "\n",
    "# Helper founction for file name giving\n",
    "def change(x):\n",
    "    if x == '(':\n",
    "        return 0\n",
    "    elif x == ')':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Check whether neigbouring bases also makes a stem brige\n",
    "def check(data):\n",
    "    num_0 = num_1 = num_2 = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i][0] == '(':\n",
    "            num_0 = num_0 + 1\n",
    "        elif data[i][0] == ')':\n",
    "            num_1 = num_1 + 1\n",
    "        else:\n",
    "            num_2 = num_2 + 1\n",
    "        if num_1 == num_0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "# Create a picture form the matrix representation, and saves them by the sliding window size\n",
    "def create_png(data, png_dir):\n",
    "    for filename, d_list in data.items():\n",
    "        if check(d_list[0]):\n",
    "            im = np.zeros([len(d_list[0])+19, len(d_list[0]), 3])\n",
    "            mat = creatmat(d_list[0])\n",
    "            im[9:len(d_list[0])+9, 0:len(d_list[0]),\n",
    "               0] = im[9:len(d_list[0])+9, 0:len(d_list[0]), 0] + mat\n",
    "            for j in range(len(d_list[0])):\n",
    "                pic = im[j:j+19]\n",
    "                image = Image.fromarray(((pic)*255/3).astype(np.uint8), 'RGB')\n",
    "                image = image.resize((169, 19))\n",
    "                new_filename = png_dir + str(change(d_list[1][j])) + '.' + filename + '_' + complete(j) + '.png'\n",
    "                image.save(new_filename)\n",
    "\n",
    "\n",
    "# Creates intermediate results from the rna sequence which later can be used for the model as data\n",
    "# rna_dir: rna sequence direcorty path to be preprocessed\n",
    "# png_dir: aim path where preprocessed images can be saved\n",
    "def pre_process_files(rna_dir, png_dir):\n",
    "    remove_pseudoknot(rna_dir)\n",
    "    data = data_extract(rna_dir)\n",
    "    create_png(data, png_dir)\n",
    "\n",
    "\n",
    "\n",
    "'''Uncomment these lines for preprocessing files\n",
    "    only needd to be done once per dicionary. \n",
    "    (The two lines corresponds for training set and evaluation set)\n",
    "    First input: rna sequnce location\n",
    "    Second input: location where intermediate results can be saved '''\n",
    "# pre_process_files('./drive/MyDrive/RNA/TEA/', './drive/MyDrive/RNA/TEA_png/')\n",
    "# pre_process_files('./drive/MyDrive/RNA/a/', './drive/MyDrive/RNA/a_png/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import shutil\n",
    "\n",
    "# Used for data normalization\n",
    "# src_dir: source directory where preprocessed images can be found\n",
    "# aim_dir: path where the normalized data can be written\n",
    "def create_eq_folders(src_dir, aim_dir):\n",
    "  pathDir = os.listdir(src_dir)\n",
    "  list_1 = []\n",
    "  list_2 = []\n",
    "  for i in range(len(pathDir)):\n",
    "    name = pathDir[i].split('.')[0]\n",
    "    if name == '2':\n",
    "        list_2.append(i)\n",
    "    else:\n",
    "        list_1.append(i)\n",
    "        dest = shutil.copyfile(src_dir+pathDir[i], aim_dir+pathDir[i])\n",
    "    \n",
    "  list_2 = sample(list_2, int(len(list_1)/2))\n",
    "  for i in list_2:\n",
    "    dest = shutil.copyfile(src_dir+pathDir[i], aim_dir+pathDir[i])\n",
    "\n",
    "\n",
    "'''Uncomment this lines for normalize files\n",
    "    only needd to be done once per dicionary. '''\n",
    "# create_eq_folders('./drive/MyDrive/RNA/a_png/', './drive/MyDrive/RNA/a_png_eq/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "acxS3T7iWVoU"
   },
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, depth=3):\n",
    "    \"\"\"\n",
    "    Computes the one hot encoding for a single label\n",
    "\n",
    "    Arguments:\n",
    "        label --  (int) Categorical labels\n",
    "        depth --  (int) Number of different classes that label can take\n",
    "\n",
    "    Returns:\n",
    "         one_hot -- tf.Tensor A single-column matrix with the one hot encoding.\n",
    "    \"\"\"\n",
    "    one_hot = tf.reshape(tf.one_hot(label, depth, axis=0), (depth,))\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def get_files(png_dir):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        png_dir:file directtory\n",
    "    Returns:\n",
    "        list of images and labels\n",
    "    \"\"\"\n",
    "    left_bracket = []\n",
    "    label_left_bracket = []\n",
    "    right_bracket = []\n",
    "    label_right_bracket = []\n",
    "    point = []\n",
    "    label_point = []\n",
    "\n",
    "    for file in os.listdir(png_dir):\n",
    "        name = file.split('.')\n",
    "        if name[0] == '0':\n",
    "            left_bracket.append(png_dir + file)\n",
    "            label_left_bracket.append(one_hot_matrix(0, depth=3))\n",
    "        elif name[0] == '1':\n",
    "            right_bracket.append(png_dir + file)\n",
    "            label_right_bracket.append(one_hot_matrix(1, depth=3))\n",
    "        else:\n",
    "            point.append(png_dir + file)\n",
    "            label_point.append(one_hot_matrix(2, depth=3))\n",
    "\n",
    "    print(\"There are %d left bracket\\nThere are %d right bracket\\nThere are %d point\" % (\n",
    "        len(left_bracket), len(right_bracket), len(point)))\n",
    "    print(len(label_left_bracket), len(label_right_bracket), len(label_point))\n",
    "\n",
    "    image_list = np.hstack((left_bracket, right_bracket, point))\n",
    "    label_list = np.vstack(\n",
    "        (label_left_bracket, label_right_bracket, label_point))\n",
    "\n",
    "    temp = list(zip(image_list, label_list))\n",
    "    temp = np.array(temp)\n",
    "    np.random.shuffle(temp)\n",
    "    temp = temp.transpose()\n",
    "\n",
    "    image_list = list(temp[0, :])\n",
    "    label_list = list(temp[1, :])\n",
    "\n",
    "    return image_list, label_list\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(image_paths, labels, batch_size, IMG_W, IMG_H, buffer_size, prefetch, shuffle):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_paths: list of image file paths\n",
    "        labels: list of corresponding image labels\n",
    "        batch_size: batch size\n",
    "        IMG_W: width of images\n",
    "        IMG_H: height of images\n",
    "        buffer_size: buffer size for shuffling\n",
    "        prefetch: number of batches prefetched\n",
    "        shuffle: True or False, if false the batches are not shuffled \n",
    "    Returns:\n",
    "        A tf.data.Dataset object containing batches of preprocessed images and labels\n",
    "    \"\"\"\n",
    "    # Define generator function to yield batches of images and labels\n",
    "    def load_and_preprocess_image(path):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        return image\n",
    "\n",
    "    \n",
    "    def generator():\n",
    "        for _ in range(1):\n",
    "            indices = [x for x in range(len(image_paths))]\n",
    "            temp = np.array(indices)\n",
    "            if shuffle:\n",
    "              np.random.shuffle(temp)\n",
    "            for i in range(0, len(image_paths), batch_size):\n",
    "                batch_indices = temp[i:i+batch_size]\n",
    "                if len(batch_indices) == batch_size:\n",
    "                  if batch_size == 1:\n",
    "                    batch_paths = itemgetter(*batch_indices)(image_paths)\n",
    "                    batch_labels = itemgetter(*batch_indices)(labels)\n",
    "                    batch_images = load_and_preprocess_image(batch_paths)\n",
    "                    batch_images = tf.expand_dims(batch_images, axis=0)\n",
    "                    batch_labels = tf.expand_dims(batch_labels, axis=0) \n",
    "                    yield batch_images, batch_labels\n",
    "                  else:\n",
    "                    batch_paths = list(itemgetter(*batch_indices)(image_paths))\n",
    "                    batch_labels = list(itemgetter(*batch_indices)(labels))\n",
    "                    batch_images = [load_and_preprocess_image(path) for path in batch_paths]\n",
    "                    yield tf.stack(batch_images), batch_labels\n",
    "\n",
    "    # Create dataset from generator function\n",
    "    data = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.float32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape([batch_size, IMG_H, IMG_W, 3]), tf.TensorShape([batch_size, 3]))\n",
    "    )\n",
    "\n",
    "    # Shuffle and repeat dataset\n",
    "    data = data.shuffle(buffer_size , reshuffle_each_iteration=True)\n",
    "    data = data.repeat(1)\n",
    "    \n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N5PNjWq6WiHH"
   },
   "outputs": [],
   "source": [
    "class InferenceModule(tf.Module):\n",
    "  '''CNN Model\n",
    "    Args:\n",
    "    n_classes: number of distinct classes to classify to\n",
    "    W: image width\n",
    "    H: image height\n",
    "    BATCH_SIZE: batch_size of input data'''\n",
    "  def __init__(self, n_classes, W, H, BATCH_SIZE):\n",
    "        self.flag = True\n",
    "        self.n_classes = n_classes\n",
    "        self.conv1_weights = tf.Variable(tf.random.truncated_normal(\n",
    "            shape=[3, 3, 3, 16], stddev=0.1), name=\"conv1_weights\")\n",
    "        self.conv1_biases = tf.Variable(tf.constant(\n",
    "            0.1, shape=[16]), name=\"conv1_biases\")\n",
    "        self.conv2_weights = tf.Variable(tf.random.truncated_normal(\n",
    "            shape=[3, 3, 16, 16], stddev=0.1), name=\"conv2_weights\")\n",
    "        self.conv2_biases = tf.Variable(tf.constant(\n",
    "            0.1, shape=[16]), name=\"conv2_biases\")\n",
    "        self.local3_biases = tf.Variable(tf.constant(\n",
    "            0.1, shape=[32]), name=\"local3_biases\")\n",
    "        self.local3_weights = tf.Variable(tf.random.truncated_normal(\n",
    "            shape=[H*W*16, 32], stddev=0.005), name=\"local3_weights\")\n",
    "        self.local4_weights = tf.Variable(tf.random.truncated_normal(\n",
    "            shape=[32, 32], stddev=0.005), name=\"local4_weights\")\n",
    "        self.local4_biases = tf.Variable(tf.constant(\n",
    "            0.1, shape=[32]), name=\"local4_biases\")\n",
    "        self.softmax_linear_weights = tf.Variable(tf.random.truncated_normal(\n",
    "            shape=[32, self.n_classes], stddev=0.005), name=\"softmax_linear_weights\")\n",
    "        self.softmax_linear_biases = tf.Variable(tf.constant(\n",
    "            0.1, shape=[self.n_classes]), name=\"softmax_linear_biases\")\n",
    "\n",
    "  def __call__(self, images, batch_size):\n",
    "        \"\"\"Build the model\n",
    "        Args:\n",
    "            image:image batch ,4D tensor, tf.float32,[batch_size,width,height,channels]\n",
    "            batch_size: batch_size of input data\n",
    "        Return:\n",
    "            output tensor with the computed logits,float,[batch_size,n_classes]\n",
    "        \"\"\"\n",
    "        # Conv 1\n",
    "        conv1_conv = tf.nn.conv2d(images, self.conv1_weights, strides=[\n",
    "                                  1, 1, 1, 1], padding=\"SAME\")\n",
    "        pre_activation = tf.nn.bias_add(conv1_conv, self.conv1_biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=\"conv1\")\n",
    "\n",
    "        # Pool 1\n",
    "        pool1 = tf.nn.avg_pool2d(conv1, ksize=[1, 3, 3, 1], strides=[\n",
    "                                 1, 1, 1, 1], padding=\"SAME\", name=\"pooling1\")\n",
    "        norm1 = tf.nn.local_response_normalization(\n",
    "            pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\"norm1\")\n",
    "\n",
    "        # Conv 2\n",
    "        conv2_conv = tf.nn.conv2d(norm1, self.conv2_weights, strides=[\n",
    "                                  1, 1, 1, 1], padding=\"SAME\")\n",
    "        pre_activation = tf.nn.bias_add(conv2_conv, self.conv2_biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=\"conv2\")\n",
    "\n",
    "        # Pool 2\n",
    "        norm2 = tf.nn.local_response_normalization(\n",
    "            conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\"norm2\")\n",
    "        pool2 = tf.nn.avg_pool2d(norm2, ksize=[1, 3, 3, 1], strides=[\n",
    "                                 1, 1, 1, 1], padding=\"SAME\", name=\"pooling2\")\n",
    "\n",
    "        # Local 3\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, self.local3_weights) +\n",
    "                            self.local3_biases, name=\"local3\")\n",
    "\n",
    "\n",
    "        # Local 4\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, self.local4_weights) +\n",
    "                            self.local4_biases, name=\"local4\")\n",
    "        \n",
    "\n",
    "        # Softmax linear\n",
    "        fully_connect = tf.add(tf.matmul(\n",
    "            local4 , self.softmax_linear_weights), self.softmax_linear_biases, name=\"softmax_linear\")\n",
    "        softmax_linear = tf.nn.softmax(fully_connect)\n",
    "        \n",
    "        return softmax_linear, fully_connect\n",
    "\n",
    "# Function to calculate prediction accuracy\n",
    "def evalution(logits, labels):\n",
    "    labels = tf.argmax(labels, axis=-1)\n",
    "    correct = tf.nn.in_top_k(labels, logits, 1)\n",
    "    correct = tf.cast(correct, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "id": "GukPJEQkXf09",
    "outputId": "c7018e38-23ec-4bc6-c7ea-61b7918d96c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 167440 left bracket\n",
      "There are 167440 right bracket\n",
      "There are 167440 point\n",
      "167440 167440 167440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkovacsdeak\\AppData\\Local\\Temp\\ipykernel_11668\\52768072.py:52: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp = np.array(temp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26541 left bracket\n",
      "There are 26541 right bracket\n",
      "There are 26541 point\n",
      "26541 26541 26541\n",
      "Epoch 000: Train loss: 1.068, Train accuracy: 42.384%\n",
      "Epoch 000: Test loss: 0.999, Test accuracy: 46.871%\n",
      "Epoch 001: Train loss: 1.022, Train accuracy: 47.595%\n",
      "Epoch 001: Test loss: 0.986, Test accuracy: 48.557%\n",
      "Epoch 002: Train loss: 1.001, Train accuracy: 49.600%\n",
      "Epoch 002: Test loss: 0.984, Test accuracy: 49.244%\n",
      "Epoch 003: Train loss: 0.976, Train accuracy: 51.882%\n",
      "Epoch 003: Test loss: 0.982, Test accuracy: 48.855%\n",
      "Epoch 004: Train loss: 0.945, Train accuracy: 54.453%\n",
      "Epoch 004: Test loss: 0.999, Test accuracy: 49.155%\n",
      "Epoch 005: Train loss: 0.911, Train accuracy: 57.054%\n",
      "Epoch 005: Test loss: 0.995, Test accuracy: 49.797%\n",
      "Epoch 006: Train loss: 0.875, Train accuracy: 59.584%\n",
      "Epoch 006: Test loss: 0.991, Test accuracy: 50.010%\n",
      "Epoch 007: Train loss: 0.842, Train accuracy: 61.863%\n",
      "Epoch 007: Test loss: 1.055, Test accuracy: 49.373%\n",
      "Epoch 008: Train loss: 0.808, Train accuracy: 64.149%\n",
      "Epoch 008: Test loss: 1.030, Test accuracy: 50.028%\n",
      "Epoch 009: Train loss: 0.777, Train accuracy: 66.229%\n",
      "Epoch 009: Test loss: 1.067, Test accuracy: 49.726%\n",
      "Epoch 010: Train loss: 0.749, Train accuracy: 67.919%\n",
      "Epoch 010: Test loss: 1.064, Test accuracy: 49.697%\n",
      "Epoch 011: Train loss: 0.723, Train accuracy: 69.455%\n",
      "Epoch 011: Test loss: 1.116, Test accuracy: 50.059%\n",
      "Epoch 012: Train loss: 0.700, Train accuracy: 70.826%\n",
      "Epoch 012: Test loss: 1.141, Test accuracy: 50.270%\n",
      "Epoch 013: Train loss: 0.679, Train accuracy: 72.127%\n",
      "Epoch 013: Test loss: 1.152, Test accuracy: 49.542%\n",
      "Epoch 014: Train loss: 0.660, Train accuracy: 73.197%\n",
      "Epoch 014: Test loss: 1.202, Test accuracy: 49.471%\n",
      "Epoch 015: Train loss: 0.642, Train accuracy: 74.247%\n",
      "Epoch 015: Test loss: 1.216, Test accuracy: 48.991%\n",
      "Epoch 016: Train loss: 0.625, Train accuracy: 75.189%\n",
      "Epoch 016: Test loss: 1.249, Test accuracy: 49.886%\n",
      "Epoch 017: Train loss: 0.610, Train accuracy: 76.105%\n",
      "Epoch 017: Test loss: 1.276, Test accuracy: 49.094%\n",
      "Epoch 018: Train loss: 0.598, Train accuracy: 76.806%\n",
      "Epoch 018: Test loss: 1.315, Test accuracy: 49.101%\n",
      "Epoch 019: Train loss: 0.583, Train accuracy: 77.607%\n",
      "Epoch 019: Test loss: 1.354, Test accuracy: 49.098%\n",
      "Epoch 020: Train loss: 0.574, Train accuracy: 78.189%\n",
      "Epoch 020: Test loss: 1.432, Test accuracy: 48.934%\n",
      "Epoch 021: Train loss: 0.558, Train accuracy: 79.010%\n",
      "Epoch 021: Test loss: 1.422, Test accuracy: 49.387%\n",
      "Epoch 022: Train loss: 0.550, Train accuracy: 79.623%\n",
      "Epoch 022: Test loss: 1.561, Test accuracy: 47.325%\n",
      "Epoch 023: Train loss: 0.540, Train accuracy: 80.140%\n",
      "Epoch 023: Test loss: 1.491, Test accuracy: 48.817%\n",
      "Epoch 024: Train loss: 0.530, Train accuracy: 80.651%\n",
      "Epoch 024: Test loss: 1.494, Test accuracy: 48.822%\n",
      "Epoch 025: Train loss: 0.523, Train accuracy: 81.209%\n",
      "Epoch 025: Test loss: 1.555, Test accuracy: 48.563%\n",
      "Epoch 026: Train loss: 0.512, Train accuracy: 81.759%\n",
      "Epoch 026: Test loss: 1.672, Test accuracy: 47.499%\n",
      "Epoch 027: Train loss: 0.505, Train accuracy: 82.138%\n",
      "Epoch 027: Test loss: 1.705, Test accuracy: 48.258%\n",
      "Epoch 028: Train loss: 0.499, Train accuracy: 82.640%\n",
      "Epoch 028: Test loss: 1.681, Test accuracy: 48.350%\n",
      "Epoch 029: Train loss: 0.491, Train accuracy: 83.078%\n",
      "Epoch 029: Test loss: 1.708, Test accuracy: 48.561%\n",
      "Epoch 030: Train loss: 0.483, Train accuracy: 83.481%\n",
      "Epoch 030: Test loss: 1.715, Test accuracy: 48.618%\n",
      "Epoch 031: Train loss: 0.478, Train accuracy: 83.894%\n",
      "Epoch 031: Test loss: 1.722, Test accuracy: 48.490%\n",
      "Epoch 032: Train loss: 0.471, Train accuracy: 84.312%\n",
      "Epoch 032: Test loss: 1.721, Test accuracy: 48.579%\n",
      "Epoch 033: Train loss: 0.464, Train accuracy: 84.662%\n",
      "Epoch 033: Test loss: 1.761, Test accuracy: 48.118%\n",
      "Epoch 034: Train loss: 0.463, Train accuracy: 84.857%\n",
      "Epoch 034: Test loss: 1.836, Test accuracy: 48.203%\n",
      "Epoch 035: Train loss: 0.455, Train accuracy: 85.395%\n",
      "Epoch 035: Test loss: 1.879, Test accuracy: 48.257%\n",
      "Epoch 036: Train loss: 0.451, Train accuracy: 85.609%\n",
      "Epoch 036: Test loss: 1.868, Test accuracy: 48.252%\n",
      "Epoch 037: Train loss: 0.445, Train accuracy: 85.977%\n",
      "Epoch 037: Test loss: 2.049, Test accuracy: 48.453%\n",
      "Epoch 038: Train loss: 0.440, Train accuracy: 86.251%\n",
      "Epoch 038: Test loss: 1.974, Test accuracy: 48.793%\n",
      "Epoch 039: Train loss: 0.435, Train accuracy: 86.479%\n",
      "Epoch 039: Test loss: 2.018, Test accuracy: 47.918%\n",
      "Epoch 040: Train loss: 0.431, Train accuracy: 86.875%\n",
      "Epoch 040: Test loss: 1.953, Test accuracy: 48.412%\n",
      "Epoch 041: Train loss: 0.431, Train accuracy: 86.974%\n",
      "Epoch 041: Test loss: 2.110, Test accuracy: 48.176%\n",
      "Epoch 042: Train loss: 0.428, Train accuracy: 87.213%\n",
      "Epoch 042: Test loss: 2.090, Test accuracy: 48.391%\n",
      "Epoch 043: Train loss: 0.422, Train accuracy: 87.485%\n",
      "Epoch 043: Test loss: 2.120, Test accuracy: 48.224%\n",
      "Epoch 044: Train loss: 0.420, Train accuracy: 87.668%\n",
      "Epoch 044: Test loss: 2.223, Test accuracy: 47.887%\n",
      "Epoch 045: Train loss: 0.415, Train accuracy: 87.952%\n",
      "Epoch 045: Test loss: 2.295, Test accuracy: 47.904%\n",
      "Epoch 046: Train loss: 0.414, Train accuracy: 88.102%\n",
      "Epoch 046: Test loss: 2.174, Test accuracy: 48.311%\n",
      "Epoch 047: Train loss: 0.412, Train accuracy: 88.302%\n",
      "Epoch 047: Test loss: 2.198, Test accuracy: 47.944%\n",
      "Epoch 048: Train loss: 0.408, Train accuracy: 88.541%\n",
      "Epoch 048: Test loss: 2.252, Test accuracy: 47.906%\n",
      "Epoch 049: Train loss: 0.404, Train accuracy: 88.775%\n",
      "Epoch 049: Test loss: 2.348, Test accuracy: 47.754%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# parameters\n",
    "N_CLASSES = 3\n",
    "IMG_W = 169\n",
    "IMG_H = 19\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 1000\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "def run_training():\n",
    "\n",
    "    logs_train_dir = './drive/MyDrive/RNA/log/'\n",
    "    \n",
    "    train, train_label = get_files('./drive/MyDrive/RNA/a_png_eq/')\n",
    "    eval, eval_label = get_files('./drive/MyDrive/RNA/a_png_eq/')\n",
    "    \n",
    "    model = InferenceModule(N_CLASSES, IMG_W, IMG_H, BATCH_SIZE)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "\n",
    "    recording = [[[], []], [[], []]]\n",
    "    for epoch in range(num_epochs):\n",
    "      epoch_cost = 0.\n",
    "      epoch_acc = 0.\n",
    "      i = 0\n",
    "      data = get_batch(train, train_label, BATCH_SIZE, IMG_W, IMG_H, CAPACITY, 2, True)\n",
    "      # Train loop\n",
    "      for batch, (images, labels) in enumerate(data):\n",
    "          i += 1\n",
    "          with tf.GradientTape() as tape:\n",
    "            softmax, train_logits = model(images, BATCH_SIZE)\n",
    "            l2_regularizer = tf.nn.l2_loss(model.conv1_weights) + tf.nn.l2_loss(model.conv2_weights) + tf.nn.l2_loss(model.local3_weights) + tf.nn.l2_loss(model.local4_weights) + tf.nn.l2_loss(model.softmax_linear_weights)\n",
    "            train_loss =  tf.reduce_mean(tf.keras.losses.categorical_crossentropy(labels, softmax)) + 0.001 * l2_regularizer\n",
    "\n",
    "          grads = tape.gradient(train_loss, model.trainable_variables)\n",
    "          grads_vars = zip(grads, model.trainable_variables)\n",
    "          optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "\n",
    "          train_acc = evalution(train_logits, labels)\n",
    "          epoch_cost += train_loss\n",
    "          epoch_acc += train_acc\n",
    "\n",
    "      test_cost = 0.\n",
    "      test_acc = 0.\n",
    "      ev_data = get_batch(eval, eval_label, BATCH_SIZE, IMG_W, IMG_H, CAPACITY, 2, True)\n",
    "      # eval loop\n",
    "      for batch_te, (images_te, labels_te) in enumerate(ev_data):\n",
    "        test_softmax, test_logits = model(tf.convert_to_tensor(images_te), BATCH_SIZE)\n",
    "        test_loss =  tf.reduce_mean(tf.keras.losses.categorical_crossentropy(labels_te, test_softmax))\n",
    "        acc = evalution(test_logits, labels_te)\n",
    "        test_cost += test_loss\n",
    "        test_acc += acc\n",
    "\n",
    "      # Calc losses and accuracies\n",
    "      recording[0][0].append(epoch_cost/(batch + 1))\n",
    "      recording[0][1].append(epoch_acc/(batch + 1))\n",
    "      recording[1][0].append(test_cost/(batch_te + 1))\n",
    "      recording[1][1].append(test_acc/(batch_te + 1))\n",
    "      # print(recording)\n",
    "      print(\"Epoch {:03d}: Train loss: {:.3f}, Train accuracy: {:.3%}\".format(epoch,\n",
    "                                                                  recording[0][0][-1],\n",
    "                                                                  recording[0][1][-1]))\n",
    "      print(\"Epoch {:03d}: Test loss: {:.3f}, Test accuracy: {:.3%}\".format(epoch,\n",
    "                                                                  recording[1][0][-1],\n",
    "                                                                  recording[1][1][-1]))\n",
    "      # save\n",
    "      if max(recording[0][1]) == recording[0][1][-1]:\n",
    "        save_path = checkpoint.save(logs_train_dir+'model')\n",
    "\n",
    "    with open(\"loss.txt\", 'w') as f:\n",
    "      for i in range(len(recording[0])):\n",
    "        f.write(str(recording[1][0][i]))\n",
    "    with open(\"accuracy.txt\", 'w') as f:\n",
    "      for i in range(len(recording[0])):\n",
    "        f.write(str(recording[1][1][i]))\n",
    "\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Nussionv algorithm for the end of the pipe line\n",
    "def Nussinov(seq, pred):\n",
    "\n",
    "  def delta(i, j):\n",
    "    if str(seq[i]) + str(seq[j]) in ('AU','UA','GC','CG', 'GU', 'UG'):\n",
    "      return pred[i][0][0] + pred[j][0][1]\n",
    "    else:\n",
    "      return pred[i][0][2] + pred[j][0][2]\n",
    "\n",
    "\n",
    "  def build_m():\n",
    "    l = len(seq)\n",
    "    matrix = [[ 0 for j in range(l)] for i in range(l)]\n",
    "    for n in range(1, l):\n",
    "      i = 0\n",
    "      for j in range(n, l):\n",
    "        value1 = matrix[i+1][j-1] + delta(i, j)\n",
    "        value2 = matrix[i+1][j] + pred[i][0][2]\n",
    "        value3 = matrix[i][j-1] + pred[i][0][2]\n",
    "        if i+1 >= j:\n",
    "          value4 = 0\n",
    "        else:\n",
    "          value4 = max([matrix[i][k] + matrix[k+1][j] for k in range(i+1,j)])\n",
    "        matrix[i][j] = max(value1, value2, value3, value4)\n",
    "        i += 1\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "  def traceback(matrix, seq, i, j, pair):\n",
    "    if i<j:\n",
    "      if matrix[i][j] == matrix[i+1][j]:\n",
    "        traceback(matrix, seq, i+1, j, pair)\n",
    "      elif matrix[i][j] == matrix[i][j-1]:\n",
    "        traceback(matrix, seq, i, j-1, pair)\n",
    "      elif matrix[i][j] == matrix[i+1][j-1] + delta(i, j):\n",
    "        pair.append([str(i)+'', str(seq[i])+'', str(j)+''])\n",
    "        traceback(matrix, seq, i+1, j-1, pair)\n",
    "      else:\n",
    "        for k in range(i+1,j):\n",
    "          if matrix[i][j] == matrix[i][k] + matrix[k+1][j]:\n",
    "            traceback(matrix, seq, i, k, pair)\n",
    "            traceback(matrix, seq, k+1, j, pair)\n",
    "            break\n",
    "\n",
    "  m = build_m()\n",
    "  pairs = []\n",
    "  traceback(m, seq, 0, len(seq)-1, pairs)\n",
    "  return pairs\n",
    "\n",
    "\n",
    "\n",
    "# modified picture creation method for test files\n",
    "def create_png_test(data, png_dir):\n",
    "    for filename, d_list in data.items():\n",
    "        current = os.path.join(png_dir, filename) + '/'\n",
    "        isExist = os.path.exists(current)\n",
    "        if not isExist:\n",
    "            os.mkdir(current)\n",
    "        if check(d_list[0]):\n",
    "            im = np.zeros([len(d_list[0])+19, len(d_list[0]), 3])\n",
    "            mat = creatmat(d_list[0])\n",
    "            im[9:len(d_list[0])+9, 0:len(d_list[0]),\n",
    "               0] = im[9:len(d_list[0])+9, 0:len(d_list[0]), 0] + mat\n",
    "            for j in range(len(d_list[0])):\n",
    "                pic = im[j:j+19]\n",
    "                image = Image.fromarray(((pic)*255/3).astype(np.uint8), 'RGB')\n",
    "                image = image.resize((169, 19))\n",
    "                new_filename = current  + str(change(d_list[1][j])) + '._' + complete(j) + '.png'\n",
    "                image.save(new_filename)\n",
    "\n",
    "\n",
    "def pre_process_testfiles(rna_dir, png_dir):\n",
    "    remove_pseudoknot(rna_dir)\n",
    "    data = data_extract(rna_dir)\n",
    "    create_png_test(data, png_dir)\n",
    "\n",
    "# different file gathering function for testing without changing up the order of bases\n",
    "def get_files_test(png_dir):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_dir:file directtory\n",
    "    Returns:\n",
    "        list of images and labels\n",
    "    \"\"\"\n",
    "    test_labels = []\n",
    "    test_pic = []\n",
    "\n",
    "    for file in os.listdir(png_dir):\n",
    "        name = file.split('.')\n",
    "        if name[0] == '0':\n",
    "            test_labels.append(one_hot_matrix(0, depth=3))\n",
    "            test_pic.append(png_dir + file)\n",
    "\n",
    "        elif name[0] == '1':\n",
    "            test_labels.append(one_hot_matrix(1, depth=3))\n",
    "            test_pic.append(png_dir + file)\n",
    "        else:\n",
    "            test_labels.append(one_hot_matrix(2, depth=3))\n",
    "            test_pic.append(png_dir + file)\n",
    "\n",
    "    return test_pic, test_labels\n",
    "\n",
    "# load checkpoint given in input as a path\n",
    "def load_model_from_checkpoint(checkpoint_path):\n",
    "    model = InferenceModule(N_CLASSES, IMG_W, IMG_H, BATCH_SIZE)\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "    checkpoint.restore(checkpoint_path).expect_partial()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_src = './drive/MyDrive/RNA/a/'\n",
    "test_dir = './drive/MyDrive/RNA/test/'\n",
    "model_loc = './drive/MyDrive/RNA/log/model-6'\n",
    "'''enough to run once preprocessing the test files'''\n",
    "# pre_process_testfiles(test_dir_src, test_dir)\n",
    "\n",
    "t = True\n",
    "N_CLASSES = 3\n",
    "IMG_W = 169\n",
    "IMG_H = 19\n",
    "BATCH_SIZE = 1\n",
    "CAPACITY = 1000\n",
    "\n",
    "# calulcating accuracy for test rnas from the output of nussiov and the softmax units of the model\n",
    "def calulate_accuracy_for_one_rna(pairs, data_colection):\n",
    "  count = 0\n",
    "  for j in range(len(data_colection[0])):\n",
    "    index = [str(data_colection[0][j]), data_colection[1][j], str(data_colection[2][j])]\n",
    "    if (index in pairs):\n",
    "      pairs.remove(index)\n",
    "      count += 1\n",
    "    elif index[2] == '0':\n",
    "      count += 1\n",
    "  count -= len(pairs)\n",
    "  return count/len(data_colection[0])\n",
    "\n",
    "# calulating testing accuracy\n",
    "if t:\n",
    "  rnas = os.listdir('./drive/MyDrive/RNA/test/')\n",
    "  loaded_model = load_model_from_checkpoint(model_loc)\n",
    "  preds = []\n",
    "  data_colection = []\n",
    "  for rna in rnas:\n",
    "      data, f = readfile(rna, test_dir_src)\n",
    "      data_colection.append(data)\n",
    "\n",
    "      preds.append([])\n",
    "      test, test_label = get_files(test_dir + rna + '/')\n",
    "      test_data = get_batch(test, test_label, BATCH_SIZE, IMG_W, IMG_H, CAPACITY, 2, False)\n",
    "      a = 0\n",
    "      for batch_te, (images_te, labels_te) in enumerate(test_data):\n",
    "        test_softmax, test_logits = loaded_model(tf.convert_to_tensor(images_te), BATCH_SIZE)\n",
    "        preds[-1].append((np.array(test_softmax)))\n",
    "\n",
    "  pairs = []\n",
    "  for i in range(len(preds)):\n",
    "    seq = data_colection[i][1]\n",
    "    pairs.append(Nussinov(seq, preds[i]))\n",
    "  acc = 0\n",
    "  for i in range(len(pairs)):\n",
    "    acc += calulate_accuracy_for_one_rna(pairs[i], data_colection[i])\n",
    "  acc /= len(pairs)\n",
    "  print('Overall acuracy during test:', acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
